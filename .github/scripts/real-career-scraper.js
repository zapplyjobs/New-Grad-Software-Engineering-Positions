const fs = require("fs");
const { generateJobId } = require("./job-fetcher/utils");
const {isUSOnlyJob} = require("./job-fetcher/utils");
const {filterJobsByLevel} =require("./job-fetcher/utils")
const { scrapeCompanyData } = require('../../jobboard/src/backend/core/scraper.js');
const { getCompanies } = require('../../jobboard/src/backend/config/companies.js');
const { transformJobs ,convertDateToRelative } = require('../../jobboard/src/backend/output/jobTransformer.js');
// Load company database
const companies = JSON.parse(
ย fs.readFileSync("./.github/scripts/job-fetcher/companies.json", "utf8")
);
const ALL_COMPANIES = Object.values(companies).flat();

const BATCH_CONFIG = {
ย batchSize: 5, ย ย ย ย ย ย ย ย ย ย// Number of scrapers to run concurrently in each batch (8 companies)
ย delayBetweenBatches: 2000, ย ย ย // Delay in milliseconds between batches (2 seconds)
ย maxRetries: 2, ย ย ย ย ย ย ย ย ย // Maximum retry attempts for failed scrapers
ย timeout: 900000, ย ย ย ย ย ย ย ย // Timeout for individual scrapers (3 minutes)
ย enableProgressBar: true, ย ย ย ย ย// Enable progress tracking
ย enableDetailedLogging: true ย ย ย// Enable detailed logging for each scraper
};

function safeISOString(dateValue) {
    console.log("Input dateValue:", dateValue);
    if (!dateValue) return new Date().toISOString();
    
    try {
        const date = new Date(dateValue);
        console.log("Parsed date:", date);
        console.log("Is valid:", !isNaN(date.getTime()));
        if (isNaN(date.getTime())) {
            console.log("Invalid date, returning current date");
            return new Date().toISOString();
        }
        return date.toISOString();
    } catch (error) {
        console.log("Error:", error);
        return new Date().toISOString();
    }
}

// Function to create custom batch configuration
function createBatchConfig(options = {}) {
ย return {
ย ย ...BATCH_CONFIG,
ย ย ...options
ย };
}
// Real career page endpoints for major companies
const CAREER_APIS = {
// ย // Greenhouse API Companies
// ย Stripe: {
// ย ย api: "https://api.greenhouse.io/v1/boards/stripe/jobs",
// ย ย method: "GET",
// ย ย parser: (data) => {
// ย ย ย if (!Array.isArray(data.jobs)) return [];
// ย ย ย return data.jobs
// ย ย ย ย .filter(
// ย ย ย ย ย (job) =>
// ย ย ย ย ย ย job.title.toLowerCase().includes("engineer") ||
// ย ย ย ย ย ย job.title.toLowerCase().includes("developer")
// ย ย ย ย )
// ย ย ย ย .map((job) => ({
// ย ย ย ย ย job_title: job.title,
// ย ย ย ย ย employer_name: "Stripe",
// ย ย ย ย ย job_city: job.location?.name?.split(", ")?.[0] || "San Francisco",
// ย ย ย ย ย job_state: job.location?.name?.split(", ")?.[1] || "CA",
// ย ย ย ย ย job_description:
// ย ย ย ย ย ย job.content ||
// ย ย ย ย ย ย "Join Stripe to help build the economic infrastructure for the internet.",
// ย ย ย ย ย job_apply_link: job.absolute_url,
// ย ย ย ย ย job_posted_at: convertDateToRelative(safeISOString(job.updated_at)),
// ย ย ย ย ย job_employment_type: "FULLTIME",
// ย ย ย ย }));
// ย ย },
// ย },

// ย Coinbase: {
// ย ย api: "https://api.greenhouse.io/v1/boards/coinbase/jobs",
// ย ย method: "GET",
// ย ย parser: (data) => {
// ย ย ย if (!Array.isArray(data.jobs)) return [];
// ย ย ย return data.jobs
// ย ย ย ย .filter(
// ย ย ย ย ย (job) =>
// ย ย ย ย ย ย job.title.toLowerCase().includes("engineer") ||
// ย ย ย ย ย ย job.title.toLowerCase().includes("developer")
// ย ย ย ย )
// ย ย ย ย .map((job) => ({
// ย ย ย ย ย job_title: job.title,
// ย ย ย ย ย employer_name: "Coinbase",
// ย ย ย ย ย job_city: job.location?.name?.split(", ")?.[0] || "San Francisco",
// ย ย ย ย ย job_state: job.location?.name?.split(", ")?.[1] || "CA",
// ย ย ย ย ย job_description:
// ย ย ย ย ย ย job.content ||
// ย ย ย ย ย ย "Join Coinbase to build the future of cryptocurrency.",
// ย ย ย ย ย job_apply_link: job.absolute_url,
// ย ย ย ย ย job_posted_at: convertDateToRelative(safeISOString(job.updated_at)),
// ย ย ย ย ย job_employment_type: "FULLTIME",
// ย ย ย ย }));
// ย ย },
// ย },

// ย Airbnb: {
// ย ย api: "https://api.greenhouse.io/v1/boards/airbnb/jobs",
// ย ย method: "GET",
// ย ย parser: (data) => {
// ย ย ย if (!Array.isArray(data.jobs)) return [];
// ย ย ย return data.jobs
// ย ย ย ย .filter(
// ย ย ย ย ย (job) =>
// ย ย ย ย ย ย job.title.toLowerCase().includes("engineer") ||
// ย ย ย ย ย ย job.title.toLowerCase().includes("developer")
// ย ย ย ย )
// ย ย ย ย .map((job) => ({
// ย ย ย ย ย job_title: job.title,
// ย ย ย ย ย employer_name: "Airbnb",
// ย ย ย ย ย job_city: job.location?.name?.split(", ")?.[0] || "San Francisco",
// ย ย ย ย ย job_state: job.location?.name?.split(", ")?.[1] || "CA",
// ย ย ย ย ย job_description:
// ย ย ย ย ย ย job.content ||
// ย ย ย ย ย ย "Join Airbnb to create a world where anyone can belong anywhere.",
// ย ย ย ย ย job_apply_link: job.absolute_url,
// ย ย ย ย ย job_posted_at: convertDateToRelative(safeISOString(job.updated_at)),
// ย ย ย ย ย job_employment_type: "FULLTIME",
// ย ย ย ย }));
// ย ย },
// ย },

// ย Databricks: {
// ย ย api: "https://api.greenhouse.io/v1/boards/databricks/jobs",
// ย ย method: "GET",
// ย ย parser: (data) => {
// ย ย ย if (!Array.isArray(data.jobs)) return [];
// ย ย ย return data.jobs
// ย ย ย ย .filter(
// ย ย ย ย ย (job) =>
// ย ย ย ย ย ย job.title.toLowerCase().includes("engineer") ||
// ย ย ย ย ย ย job.title.toLowerCase().includes("developer")
// ย ย ย ย )
// ย ย ย ย .map((job) => ({
// ย ย ย ย ย job_title: job.title,
// ย ย ย ย ย employer_name: "Databricks",
// ย ย ย ย ย job_city: job.location?.name?.split(", ")?.[0] || "San Francisco",
// ย ย ย ย ย job_state: job.location?.name?.split(", ")?.[1] || "CA",
// ย ย ย ย ย job_description:
// ย ย ย ย ย ย job.content || "Join Databricks to unify analytics and AI.",
// ย ย ย ย ย job_apply_link: job.absolute_url,
// ย ย ย ย ย job_posted_at: convertDateToRelative(safeISOString(job.updated_at)),
// ย ย ย ย ย job_employment_type: "FULLTIME",
// ย ย ย ย }));
// ย ย },
// ย },

// ย Figma: {
// ย ย api: "https://api.greenhouse.io/v1/boards/figma/jobs",
// ย ย method: "GET",
// ย ย parser: (data) => {
// ย ย ย if (!Array.isArray(data.jobs)) return [];
// ย ย ย return data.jobs
// ย ย ย ย .filter(
// ย ย ย ย ย (job) =>
// ย ย ย ย ย ย job.title.toLowerCase().includes("engineer") ||
// ย ย ย ย ย ย job.title.toLowerCase().includes("developer")
// ย ย ย ย )
// ย ย ย ย .map((job) => ({
// ย ย ย ย ย job_title: job.title,
// ย ย ย ย ย employer_name: "Figma",
// ย ย ย ย ย job_city: job.location?.name?.split(", ")?.[0] || "San Francisco",
// ย ย ย ย ย job_state: job.location?.name?.split(", ")?.[1] || "CA",
// ย ย ย ย ย job_description:
// ย ย ย ย ย ย job.content || "Join Figma to make design accessible to all.",
// ย ย ย ย ย job_apply_link: job.absolute_url,
// ย ย ย ย ย job_posted_at: convertDateToRelative(safeISOString(job.updated_at)),
// ย ย ย ย ย job_employment_type: "FULLTIME",
// ย ย ย ย }));
// ย ย },
// ย },

// ย // Custom API Companies
// ย Apple: {
// ย ย api: "https://jobs.apple.com/api/v1/search",
// ย ย method: "POST",
// ย ย headers: {
// ย ย ย "Content-Type": "application/json",
// ย ย },
// ย ย body: JSON.stringify({
// ย ย ย query: "engineer",
// ย ย ย filters: {
// ย ย ย ย locations: ["postLocation-USA"],
// ย ย ย },
// ย ย ย page: 1,
// ย ย ย locale: "en-us",
// ย ย ย sort: "newest",
// ย ย ย format: {
// ย ย ย ย longDate: "MMMM D, YYYY",
// ย ย ย ย mediumDate: "MMM D, YYYY",
// ย ย ย },
// ย ย }),
// ย ย parser: (data) => {
// ย ย ย if (!data.searchResults) return [];
// ย ย ย return data.searchResults.slice(0, 20).map((job) => ({
// ย ย ย ย job_title: job.postingTitle,
// ย ย ย ย employer_name: "Apple",
// ย ย ย ย job_city: job.locations?.[0]?.name?.split(", ")?.[0] || "Cupertino",
// ย ย ย ย job_state: job.locations?.[0]?.name?.split(", ")?.[1] || "CA",
// ย ย ย ย job_description:
// ย ย ย ย ย job.jobSummary || "Join Apple to create products that change lives.",
// ย ย ย ย job_apply_link: `https://jobs.apple.com/en-us/details/${job.positionId}`,
// ย ย ย ย job_posted_at: convertDateToRelative(job.postDateInGMT),
// ย ย ย ย job_employment_type: "FULLTIME",
// ย ย ย }));
// ย ย },
// ย },

// ย Microsoft: {
// ย ย api: "https://gcsservices.careers.microsoft.com/search/api/v1/search?l=en_us&pg=1&pgSz=20&o=Recent&flt=true",
// ย ย method: "GET",
// ย ย parser: (data) => {
// ย ย ย if (!data.operationResult?.result?.jobs) return [];
// ย ย ย return data.operationResult.result.jobs
// ย ย ย ย .filter(
// ย ย ย ย ย (job) =>
// ย ย ย ย ย ย job.title.toLowerCase().includes("engineer") ||
// ย ย ย ย ย ย job.title.toLowerCase().includes("developer")
// ย ย ย ย )
// ย ย ย ย .map((job) => ({
// ย ย ย ย ย job_title: job.title,
// ย ย ย ย ย employer_name: "Microsoft",
// ย ย ย ย ย job_city: job.primaryLocation?.city || "Redmond",
// ย ย ย ย ย job_state: job.primaryLocation?.state || "WA",
// ย ย ย ย ย job_description:
// ย ย ย ย ย ย job.description ||
// ย ย ย ย ย ย "Join Microsoft to empower every person and organization on the planet.",
// ย ย ย ย ย job_apply_link: `https://jobs.careers.microsoft.com/global/en/job/${job.jobId}`,
// ย ย ย ย ย job_posted_at: convertDateToRelative(job.postedDate),
// ย ย ย ย ย job_employment_type: "FULLTIME",
// ย ย ย ย }));
// ย ย },
// ย },

// ย // Amazon now uses dedicated scraper

// ย Netflix: {
// ย ย api: "https://explore.jobs.netflix.net/api/apply/v2/jobs?domain=netflix.com&query=engineer",
// ย ย method: "GET",
// ย ย pagination: true,
// ย ย parser: (data) => {
// ย ย ย if (!data.positions) return [];

// ย ย ย // Filter for fresh jobs from past week and engineering roles
// ย ย ย const oneWeekAgo = Date.now() - 7 * 24 * 60 * 60 * 1000;

// ย ย ย return data.positions
// ย ย ย ย .filter((job) => {
// ย ย ย ย ย const isEngineering =
// ย ย ย ย ย ย job.name.toLowerCase().includes("engineer") ||
// ย ย ย ย ย ย job.name.toLowerCase().includes("developer") ||
// ย ย ย ย ย ย job.department === "Engineering";
// ย ย ย ย ย const isFresh = job.t_create * 1000 > oneWeekAgo;
// ย ย ย ย ย return isEngineering && isFresh;
// ย ย ย ย })
// ย ย ย ย .map((job) => ({
// ย ย ย ย ย job_title: job.name,
// ย ย ย ย ย employer_name: "Netflix",
// ย ย ย ย ย job_city: job.location?.split(",")?.[0] || "Los Gatos",
// ย ย ย ย ย job_state: job.location?.split(", ")?.[1] || "CA",
// ย ย ย ย ย job_description:
// ย ย ย ย ย ย job.job_description || "Join Netflix to entertain the world.",
// ย ย ย ย ย job_apply_link: job.canonicalPositionUrl,
// ย ย ย ย ย job_posted_at: convertDateToRelative(job.t_create * 1000),
// ย ย ย ย ย job_employment_type: "FULLTIME",
// ย ย ย ย }));
// ย ย },
// ย },

// ย Qualcomm: {
// ย ย api: "https://careers.qualcomm.com/api/apply/v2/jobs?domain=qualcomm.com&num=20&query=USA&sort_by=relevance",
// ย ย method: "GET",
// ย ย parser: (data) => {
// ย ย ย if (!data.positions) return [];
// ย ย ย return data.positions
// ย ย ย ย .filter(
// ย ย ย ย ย (job) =>
// ย ย ย ย ย ย job.name.toLowerCase().includes("engineer") ||
// ย ย ย ย ย ย job.name.toLowerCase().includes("developer")
// ย ย ย ย )
// ย ย ย ย .map((job) => ({
// ย ย ย ย ย job_title: job.name,
// ย ย ย ย ย employer_name: "Qualcomm",
// ย ย ย ย ย job_city: job.location?.split(", ")?.[0] || "San Diego",
// ย ย ย ย ย job_state: job.location?.split(", ")?.[1] || "CA",
// ย ย ย ย ย job_description:
// ย ย ย ย ย ย job.description ||
// ย ย ย ย ย ย "Join Qualcomm to invent breakthrough technologies.",
// ย ย ย ย ย job_apply_link: job.canonicalPositionUrl,
// ย ย ย ย ย job_posted_at: convertDateToRelative(job.publishedDate),
// ย ย ย ย ย job_employment_type: "FULLTIME",
// ย ย ย ย }));
// ย ย },
// ย },

// ย PayPal: {
// ย ย api: "https://paypal.eightfold.ai/api/apply/v2/jobs?domain=paypal.com&num=20&location=USA&sort_by=relevance",
// ย ย method: "GET",
// ย ย parser: (data) => {
// ย ย ย if (!data.positions) return [];
// ย ย ย return data.positions
// ย ย ย ย .filter(
// ย ย ย ย ย (job) =>
// ย ย ย ย ย ย job.name.toLowerCase().includes("engineer") ||
// ย ย ย ย ย ย job.name.toLowerCase().includes("developer")
// ย ย ย ย )
// ย ย ย ย .map((job) => ({
// ย ย ย ย ย job_title: job.name,
// ย ย ย ย ย employer_name: "PayPal",
// ย ย ย ย ย job_city: job.location?.split(", ")?.[0] || "San Jose",
// ย ย ย ย ย job_state: job.location?.split(", ")?.[1] || "CA",
// ย ย ย ย ย job_description:
// ย ย ย ย ย ย job.description || "Join PayPal to democratize financial services.",
// ย ย ย ย ย job_apply_link: job.canonicalPositionUrl,
// ย ย ย ย ย job_posted_at: convertDateToRelative(job.publishedDate),
// ย ย ย ย ย job_employment_type: "FULLTIME",
// ย ย ย ย }));
// ย ย },
// ย },

// ย // Lever API Companies
// ย // Uber now uses dedicated scraper

// ย Discord: {
// ย ย api: "https://boards-api.greenhouse.io/v1/boards/discord/jobs",
// ย ย method: "GET",
// ย ย parser: (data) => {
// ย ย ย if (!Array.isArray(data.jobs)) return [];
// ย ย ย return data.jobs
// ย ย ย ย .filter(
// ย ย ย ย ย (job) =>
// ย ย ย ย ย ย job.title.toLowerCase().includes("engineer") ||
// ย ย ย ย ย ย job.title.toLowerCase().includes("developer")
// ย ย ย ย )
// ย ย ย ย .map((job) => ({
// ย ย ย ย ย job_title: job.title,
// ย ย ย ย ย employer_name: "Discord",
// ย ย ย ย ย job_city: job.location?.name?.split(", ")?.[0] || "San Francisco",
// ย ย ย ย ย job_state: job.location?.name?.split(", ")?.[1] || "CA",
// ย ย ย ย ย job_description: job.content || "Join Discord to build connections.",
// ย ย ย ย ย job_apply_link: job.absolute_url,
// ย ย ย ย ย job_posted_at: convertDateToRelative(safeISOString(job.updated_at)),
// ย ย ย ย ย job_employment_type: "FULLTIME",
// ย ย ย ย }));
// ย ย },
// ย },

// ย Lyft: {
// ย ย api: "https://boards-api.greenhouse.io/v1/boards/lyft/jobs",
// ย ย method: "GET",
// ย ย parser: (data) => {
// ย ย ย if (!Array.isArray(data.jobs)) return [];
// ย ย ย return data.jobs
// ย ย ย ย .filter(
// ย ย ย ย ย (job) =>
// ย ย ย ย ย ย job.title.toLowerCase().includes("engineer") ||
// ย ย ย ย ย ย job.title.toLowerCase().includes("developer")
// ย ย ย ย )
// ย ย ย ย .map((job) => ({
// ย ย ย ย ย job_title: job.title,
// ย ย ย ย ย employer_name: "Lyft",
// ย ย ย ย ย job_city: job.location?.name?.split(", ")?.[0] || "San Francisco",
// ย ย ย ย ย job_state: job.location?.name?.split(", ")?.[1] || "CA",
// ย ย ย ย ย job_description:
// ย ย ย ย ย ย job.content ||
// ย ย ย ย ย ย "Join Lyft to improve people's lives with the world's best transportation.",
// ย ย ย ย ย job_apply_link: job.absolute_url,
// ย ย ย ย ย job_posted_at: convertDateToRelative(safeISOString(job.updated_at)),
// ย ย ย ย ย job_employment_type: "FULLTIME",
// ย ย ย ย }));
// ย ย },
// ย },

// ย // Slack now uses dedicated scraper
};

// Utility functions
function delay(ms) {
ย return new Promise((resolve) => setTimeout(resolve, ms));
}

// convertDateToRelative is imported from jobTransformer.js// Fetch jobs from a specific company's career API
async function fetchCompanyJobs(companyName) {
ย const config = CAREER_APIS[companyName];
ย if (!config) {
ย ย console.log(`โ๏ธ No API config for ${companyName}`);
ย ย return [];
ย }

ย try {
ย ย console.log(`๐ Fetching jobs from ${companyName}...`);

ย ย const options = {
ย ย ย method: config.method,
ย ย ย headers: {
ย ย ย ย "User-Agent":
ย ย ย ย ย "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
ย ย ย ย Accept: "application/json",
ย ย ย ย ...config.headers,
ย ย ย },
ย ย };

ย ย if (config.body) {
ย ย ย options.body = config.body;
ย ย }

ย ย const response = await fetch(config.api, options);

ย ย if (!response.ok) {
ย ย ย console.log(`โ ${companyName} API returned ${response.status}`);
ย ย ย return [];
ย ย }

ย ย const data = await response.json();
ย ย const jobs = config.parser(data);

ย ย console.log(`โ Found ${jobs.length} jobs at ${companyName}`);
ย ย return jobs;
ย } catch (error) {
ย ย console.error(`โ Error fetching ${companyName} jobs:`, error.message);
ย ย return [];
ย }
}

// No sample jobs - only real API data

// Fetch jobs from SimplifyJobs public data
async function fetchSimplifyJobsData() {
ย try {
ย ย console.log("๐ก Fetching data from public sources...");

ย ย const newGradUrl =
ย ย ย "https://raw.githubusercontent.com/SimplifyJobs/New-Grad-Positions/dev/.github/scripts/listings.json";
ย ย const response = await fetch(newGradUrl);

ย ย if (!response.ok) {
ย ย ย console.log(`โ๏ธ Could not fetch external data: ${response.status}`);
ย ย ย return [];
ย ย }

ย ย const data = await response.json();

ย ย const activeJobs = data
ย ย ย .filter(
ย ย ย ย (job) =>
ย ย ย ย ย job.active &&
ย ย ย ย ย job.url &&
ย ย ย ย ย (job.title.toLowerCase().includes("engineer") ||
ย ย ย ย ย ย job.title.toLowerCase().includes("developer"))
ย ย ย )
ย ย ย .map((job) => ({
ย ย ย ย job_title: job.title,
ย ย ย ย employer_name: job.company_name,
ย ย ย ย job_city: job.locations?.[0]?.split(", ")?.[0] || "Multiple",
ย ย ย ย job_state: job.locations?.[0]?.split(", ")?.[1] || "Locations",
ย ย ย ย job_description: `Join ${job.company_name} in this exciting opportunity.`,
ย ย ย ย job_apply_link: job.url,
ย ย ย ย job_posted_at: convertDateToRelative(safeISOString(job.date_posted * 1000)),
ย ย ย ย job_employment_type: "FULLTIME",
ย ย ย }));

ย ย console.log(
ย ย ย `๐ Found ${activeJobs.length} active positions from external sources`
ย ย );
ย ย return activeJobs;
ย } catch (error) {
ย ย console.error(`โ Error fetching external data:`, error.message);
ย ย return [];
ย }
}

// Fetch jobs from all companies with real career API
async function fetchAllRealJobs(searchQuery = 'software engineering', maxPages = 1, batchConfig = BATCH_CONFIG) {
ย console.log("๐ Starting REAL career page scraping...");

ย let allJobs = [];
ย const companies = getCompanies(searchQuery);
ย const companyKeys = Object.keys(companies);

ย // Add execution tracking to prevent loops
ย const executionId = Date.now();
ย console.log(`๐ Execution ID: ${executionId}`);

ย // Define scraper configurations for batch processing
ย const scraperConfigs = companyKeys.map(companyKey => ({
ย ย name: companies[companyKey].name,
ย ย companyKey: companyKey,
ย ย scraper: () => scrapeCompanyData(companyKey, searchQuery, maxPages),
ย ย query: searchQuery,
ย ย executionId // Add execution ID to track this run
ย }));

ย // Enhanced batch processing function with comprehensive tracking and error handling
ย async function processScrapersInBatches(configs, config = batchConfig) {
ย ย const results = [];
ย ย const totalBatches = Math.ceil(configs.length / config.batchSize);
ย ย const processedCompanies = new Set(); // Track processed companies to prevent duplicates

ย ย // Enhanced tracking objects
ย ย const overallProgress = {
ย ย ย totalCompanies: configs.length,
ย ย ย processedCompanies: 0,
ย ย ย successfulCompanies: 0,
ย ย ย failedCompanies: 0,
ย ย ย skippedCompanies: 0,
ย ย ย totalJobsCollected: 0,
ย ย ย startTime: Date.now(),
ย ย ย batchResults: []
ย ย };

ย ย const companiesStatus = {
ย ย ย successful: [],
ย ย ย failed: [],
ย ย ย skipped: []
ย ย };

ย ย console.log(`๐ Starting optimized batch processing:`);
ย ย console.log(` ย ๐ Total scrapers: ${configs.length}`);
ย ย console.log(` ย ๐ฆ Batch size: ${config.batchSize} companies per batch`);
ย ย console.log(` ย โฑ๏ธ ยTotal batches: ${totalBatches}`);
ย ย console.log(` ย โณ Delay between batches: ${config.delayBetweenBatches}ms`);
ย ย console.log(` ย ๐ Max retries: ${config.maxRetries}`);
ย ย console.log(` ย ๐ Started at: ${new Date().toLocaleTimeString()}`);

ย ย for (let i = 0; i < configs.length; i += config.batchSize) {
ย ย ย const batch = configs.slice(i, i + config.batchSize);
ย ย ย const batchNumber = Math.floor(i / config.batchSize) + 1;
ย ย ย const batchStartTime = Date.now();

ย ย ย console.log(`\n๐ฆ Processing Batch ${batchNumber}/${totalBatches}: ${batch.map(c => c.name).join(', ')}`);

ย ย ย // Filter out already processed companies
ย ย ย const filteredBatch = batch.filter(scraperConfig => {
ย ย ย ย if (processedCompanies.has(scraperConfig.companyKey)) {
ย ย ย ย ย console.log(`โ๏ธ Skipping already processed company: ${scraperConfig.name}`);
ย ย ย ย ย companiesStatus.skipped.push(scraperConfig.name);
ย ย ย ย ย overallProgress.skippedCompanies++;
ย ย ย ย ย return false;
ย ย ย ย }
ย ย ย ย processedCompanies.add(scraperConfig.companyKey);
ย ย ย ย return true;
ย ย ย });

ย ย ย if (filteredBatch.length === 0) {
ย ย ย ย console.log(`โญ๏ธ Skipping batch ${batchNumber} - all companies already processed`);
ย ย ย ย continue;
ย ย ย }

ย ย ย // Batch-level tracking
ย ย ย const batchProgress = {
ย ย ย ย batchNumber,
ย ย ย ย companies: filteredBatch.map(c => c.name),
ย ย ย ย successful: [],
ย ย ย ย failed: [],
ย ย ย ย totalJobs: 0,
ย ย ย ย duration: 0,
ย ย ย ย startTime: batchStartTime
ย ย ย };

ย ย ย // Process current batch concurrently with retry logic
ย ย ย const batchPromises = filteredBatch.map(async (scraperConfig) => {
ย ย ย ย let lastError = null;
ย ย ย ย let startTime = Date.now();

ย ย ย ย for (let attempt = 1; attempt <= config.maxRetries; attempt++) {
ย ย ย ย ย try {
ย ย ย ย ย ย // Update startTime for each attempt
ย ย ย ย ย ย startTime = Date.now();

ย ย ย ย ย ย let jobs;
ย ย ย ย ย ย if (config.timeout > 0) {
ย ย ย ย ย ย ย // Timeout enabled
ย ย ย ย ย ย ย const timeoutPromise = new Promise((_, reject) => {
ย ย ย ย ย ย ย ย setTimeout(() => reject(new Error('Scraper timeout')), config.timeout);
ย ย ย ย ย ย ย });

ย ย ย ย ย ย ย jobs = await Promise.race([
ย ย ย ย ย ย ย ย scraperConfig.scraper(),
ย ย ย ย ย ย ย ย timeoutPromise
ย ย ย ย ย ย ย ]);
ย ย ย ย ย ย } else {
ย ย ย ย ย ย ย // No timeout - wait indefinitely for the scraper to complete
ย ย ย ย ย ย ย jobs = await scraperConfig.scraper();
ย ย ย ย ย ย }

ย ย ย ย ย ย const duration = Date.now() - startTime;
ย ย ย ย ย ย overallProgress.processedCompanies++;
ย ย ย ย ย ย overallProgress.successfulCompanies++;
ย ย ย ย ย ย overallProgress.totalJobsCollected += jobs?.length || 0;

ย ย ย ย ย ย // Track successful company
ย ย ย ย ย ย const successInfo = {
ย ย ย ย ย ย ย name: scraperConfig.name,
ย ย ย ย ย ย ย jobs: jobs?.length || 0,
ย ย ย ย ย ย ย duration,
ย ย ย ย ย ย ย attempts: attempt
ย ย ย ย ย ย };
ย ย ย ย ย ย companiesStatus.successful.push(successInfo);
ย ย ย ย ย ย batchProgress.successful.push(successInfo);
ย ย ย ย ย ย batchProgress.totalJobs += jobs?.length || 0;

ย ย ย ย ย ย if (config.enableDetailedLogging) {
ย ย ย ย ย ย ย console.log(`โ ${scraperConfig.name}: ${jobs?.length || 0} jobs in ${duration}ms (Attempt ${attempt})`);
ย ย ย ย ย ย }

ย ย ย ย ย ย return {
ย ย ย ย ย ย ย name: scraperConfig.name,
ย ย ย ย ย ย ย companyKey: scraperConfig.companyKey,
ย ย ย ย ย ย ย jobs: jobs || [],
ย ย ย ย ย ย ย duration,
ย ย ย ย ย ย ย success: true,
ย ย ย ย ย ย ย attempts: attempt,
ย ย ย ย ย ย ย error: null
ย ย ย ย ย ย };

ย ย ย ย ย } catch (error) {
ย ย ย ย ย ย lastError = error;
ย ย ย ย ย ย if (config.enableDetailedLogging) {
ย ย ย ย ย ย ย console.log(`โ๏ธ ย${scraperConfig.name} attempt ${attempt} failed: ${error.message}`);
ย ย ย ย ย ย }

ย ย ย ย ย ย // If this is the last attempt, mark as failed
ย ย ย ย ย ย if (attempt === config.maxRetries) {
ย ย ย ย ย ย ย const duration = Date.now() - startTime;
ย ย ย ย ย ย ย overallProgress.processedCompanies++;
ย ย ย ย ย ย ย overallProgress.failedCompanies++;

ย ย ย ย ย ย ย // Track failed company
ย ย ย ย ย ย ย const failInfo = {
ย ย ย ย ย ย ย ย name: scraperConfig.name,
ย ย ย ย ย ย ย ย error: error.message,
ย ย ย ย ย ย ย ย duration,
ย ย ย ย ย ย ย ย attempts: attempt
ย ย ย ย ย ย ย };
ย ย ย ย ย ย ย companiesStatus.failed.push(failInfo);
ย ย ย ย ย ย ย batchProgress.failed.push(failInfo);

ย ย ย ย ย ย ย console.error(`โ ${scraperConfig.name} failed after ${config.maxRetries} attempts: ${error.message}. Skipping company.`);

ย ย ย ย ย ย ย return {
ย ย ย ย ย ย ย ย name: scraperConfig.name,
ย ย ย ย ย ย ย ย companyKey: scraperConfig.companyKey,
ย ย ย ย ย ย ย ย jobs: [],
ย ย ย ย ย ย ย ย duration: duration,
ย ย ย ย ย ย ย ย success: false,
ย ย ย ย ย ย ย ย attempts: attempt,
ย ย ย ย ย ย ย ย error: error.message
ย ย ย ย ย ย ย };
ย ย ย ย ย ย }

ย ย ย ย ย ย // Exponential backoff with jitter for retry delay
ย ย ย ย ย ย const baseDelay = 2000 * Math.pow(2, attempt - 1);
ย ย ย ย ย ย const jitter = Math.random() * 1000; // Add jitter to avoid thundering herd
ย ย ย ย ย ย const retryDelay = Math.min(baseDelay + jitter, 10000); // Max 10s
ย ย ย ย ย ย if (config.enableDetailedLogging) {
ย ย ย ย ย ย ย console.log(`โณ Retrying ${scraperConfig.name} in ${retryDelay.toFixed(0)}ms...`);
ย ย ย ย ย ย }
ย ย ย ย ย ย await new Promise(resolve => setTimeout(resolve, retryDelay));
ย ย ย ย ย }
ย ย ย ย }
ย ย ย });

ย ย ย // Wait for current batch to complete, with error tolerance (continue on individual failures)
ย ย ย let batchResults;
ย ย ย try {
ย ย ย ย batchResults = await Promise.all(batchPromises);
ย ย ย } catch (batchError) {
ย ย ย ย console.error(`โ Batch ${batchNumber} had an unhandled error: ${batchError.message}. Continuing with available results.`);
ย ย ย ย batchResults = []; // Or collect partial if using allSettled
ย ย ย }
ย ย ย results.push(...batchResults.filter(result => result)); // Filter nulls if any

ย ย ย // Complete batch tracking
ย ย ย batchProgress.duration = Date.now() - batchStartTime;
ย ย ย overallProgress.batchResults.push(batchProgress);

ย ย ย // Enhanced progress reporting after each batch
ย ย ย const progressPercent = ((overallProgress.processedCompanies / overallProgress.totalCompanies) * 100).toFixed(1);
ย ย ย const elapsedTime = Date.now() - overallProgress.startTime;
ย ย ย const avgTimePerCompany = overallProgress.processedCompanies > 0 ? elapsedTime / overallProgress.processedCompanies : 0;
ย ย ย const estimatedTimeRemaining = avgTimePerCompany * (overallProgress.totalCompanies - overallProgress.processedCompanies);

ย ย ย console.log(`\n๐ Batch ${batchNumber}/${totalBatches} Completed in ${(batchProgress.duration/1000).toFixed(1)}s:`);
ย ย ย console.log(` ย โ Successful: ${batchProgress.successful.length} companies`);
ย ย ย console.log(` ย โ Failed: ${batchProgress.failed.length} companies`);
ย ย ย console.log(` ย ๐ Jobs collected in this batch: ${batchProgress.totalJobs}`);

ย ย ย if (batchProgress.successful.length > 0) {
ย ย ย ย console.log(` ย ๐ฏ Successful companies: ${batchProgress.successful.map(s => `${s.name}(${s.jobs})`).join(', ')}`);
ย ย ย }

ย ย ย if (batchProgress.failed.length > 0) {
ย ย ย ย console.log(` ย ๐ฅ Failed companies: ${batchProgress.failed.map(f => `${f.name}(${f.error.substring(0, 30)}...)`).join(', ')}`);
ย ย ย }

ย ย ย console.log(`\n๐ Overall Progress: ${overallProgress.processedCompanies}/${overallProgress.totalCompanies} (${progressPercent}%)`);
ย ย ย console.log(` ย โ Total Successful: ${overallProgress.successfulCompanies}`);
ย ย ย console.log(` ย โ Total Failed: ${overallProgress.failedCompanies}`);
ย ย ย console.log(` ย โญ๏ธ ยTotal Skipped: ${overallProgress.skippedCompanies}`);
ย ย ย console.log(` ย ๐ Total Jobs Collected: ${overallProgress.totalJobsCollected}`);
ย ย ย console.log(` ย โฑ๏ธ ยElapsed Time: ${(elapsedTime/1000).toFixed(1)}s`);
ย ย ย console.log(` ย ๐ฎ Estimated Time Remaining: ${(estimatedTimeRemaining/1000).toFixed(1)}s`);

ย ย ย // Add delay between batches (except for the last batch)
ย ย ย if (i + config.batchSize < configs.length) {
ย ย ย ย console.log(`โณ Waiting ${config.delayBetweenBatches}ms before next batch...`);
ย ย ย ย await new Promise(resolve => setTimeout(resolve, config.delayBetweenBatches));
ย ย ย }
ย ย }

ย ย // Final comprehensive summary
ย ย const totalDuration = Date.now() - overallProgress.startTime;
ย ย console.log(`\n๐ ===== BATCH PROCESSING COMPLETE =====`);
ย ย console.log(`๐ Total Duration: ${(totalDuration/1000).toFixed(1)}s (${(totalDuration/60000).toFixed(1)} minutes)`);
ย ย console.log(`๐ Final Statistics:`);
ย ย console.log(` ย ๐ Total Companies Processed: ${overallProgress.processedCompanies}/${overallProgress.totalCompanies}`);
ย ย console.log(` ย โ Successful Companies: ${overallProgress.successfulCompanies} (${((overallProgress.successfulCompanies/overallProgress.totalCompanies)*100).toFixed(1)}%)`);
ย ย console.log(` ย โ Failed Companies: ${overallProgress.failedCompanies} (${((overallProgress.failedCompanies/overallProgress.totalCompanies)*100).toFixed(1)}%)`);
ย ย console.log(` ย โญ๏ธ ยSkipped Companies: ${overallProgress.skippedCompanies} (${((overallProgress.skippedCompanies/overallProgress.totalCompanies)*100).toFixed(1)}%)`);
ย ย console.log(` ย ๐ Total Jobs Collected: ${overallProgress.totalJobsCollected}`);
ย ย console.log(` ย โก Average Jobs per Successful Company: ${overallProgress.successfulCompanies > 0 ? (overallProgress.totalJobsCollected/overallProgress.successfulCompanies).toFixed(1) : 0}`);

ย ย // Detailed success and failure breakdown
ย ย console.log(`\n๐ Successful Companies (${companiesStatus.successful.length}):`);
ย ย companiesStatus.successful
ย ย ย .sort((a, b) => b.jobs - a.jobs) // Sort by job count descending
ย ย ย .forEach((company, index) => {
ย ย ย ย console.log(` ย ${index + 1}. ${company.name}: ${company.jobs} jobs (${(company.duration/1000).toFixed(1)}s, ${company.attempts} attempts)`);
ย ย ย });

ย ย if (companiesStatus.failed.length > 0) {
ย ย ย console.log(`\n๐ฅ Failed Companies (${companiesStatus.failed.length}):`);
ย ย ย companiesStatus.failed.forEach((company, index) => {
ย ย ย ย console.log(` ย ${index + 1}. ${company.name}: ${company.error} (${(company.duration/1000).toFixed(1)}s, ${company.attempts} attempts)`);
ย ย ย });
ย ย }

ย ย if (companiesStatus.skipped.length > 0) {
ย ย ย console.log(`\nโญ๏ธ Skipped Companies (${companiesStatus.skipped.length}):`);
ย ย ย companiesStatus.skipped.forEach((company, index) => {
ย ย ย ย console.log(` ย ${index + 1}. ${company}`);
ย ย ย });
ย ย }

ย ย console.log(`๐ Batch processing completed. Total results: ${results.length}`);
ย ย return results;
ย }

ย // Process all scrapers in optimized batches
ย const batchResults = await processScrapersInBatches(scraperConfigs, batchConfig);

ย // Collect all jobs from successful scrapers and transform immediately
ย const processedJobIds = new Set(); // Track processed job IDs to prevent duplicates

ย batchResults.forEach(result => {
ย ย if (result.success && result.jobs && result.jobs.length > 0) {
ย ย ย try {
ย ย ย ย const transformedJobs = transformJobs(result.jobs, searchQuery);
ย ย ย ย console.log(`๐ Transforming ${result.jobs.length} jobs from ${result.name}`);

ย ย ย ย // Filter out already processed jobs
ย ย ย ย const newJobs = transformedJobs.filter(job => {
ย ย ย ย ย const jobId = generateJobId(job);
ย ย ย ย ย if (processedJobIds.has(jobId)) {
ย ย ย ย ย ย return false;
ย ย ย ย ย }
ย ย ย ย ย processedJobIds.add(jobId);
ย ย ย ย ย return true;
ย ย ย ย });

ย ย ย ย if (newJobs.length > 0) {
ย ย ย ย ย allJobs.push(...newJobs);
ย ย ย ย ย console.log(`โ Added ${newJobs.length} new jobs from ${result.name} (${transformedJobs.length - newJobs.length} duplicates filtered)`);
ย ย ย ย } else {
ย ย ย ย ย console.log(`โ๏ธ No new jobs from ${result.name} - all were duplicates`);
ย ย ย ย }
ย ย ย } catch (transformError) {
ย ย ย ย console.error(`โ Error transforming jobs from ${result.name}:`, transformError.message);
ย ย ย }
ย ย } else if (result.success) {
ย ย ย console.log(`โน๏ธ ${result.name} returned no jobs`);
ย ย }
ย });

ย console.log(`๐ Total scraped jobs collected after transformation: ${allJobs.length}`);

ย // Early exit if no jobs found
ย if (allJobs.length === 0) {
ย ย console.log(`โ๏ธ No scraped jobs found. Will only collect API jobs.`);
ย }

ย // Filter jobs by level (remove senior-level positions) BEFORE adding API/external jobs
ย console.log('๐ฏ Filtering scraped jobs by experience level...');
ย let levelFilteredJobs = [];
ย try {
ย ย if (allJobs.length > 0) {
ย ย ย levelFilteredJobs = filterJobsByLevel(allJobs);
ย ย ย console.log(`๐ฏ Level filtering: ${allJobs.length} -> ${levelFilteredJobs.length} scraped jobs`);
ย ย }
ย } catch (filterError) {
ย ย console.error('โ Error in level filtering:', filterError.message);
ย ย levelFilteredJobs = allJobs; // Fallback to unfiltered jobs
ย }

ย // Filter out non-US jobs from scraped jobs
ย const removedJobs = [];
ย const initialScrapedCount = levelFilteredJobs.length;

ย try {
ย ย if (levelFilteredJobs.length > 0) {
ย ย ย levelFilteredJobs = levelFilteredJobs.filter(job => {
ย ย ย ย const isUSJob = isUSOnlyJob(job);

ย ย ย ย if (!isUSJob) {
ย ย ย ย ย removedJobs.push(job);
ย ย ย ย ย return false; // Remove non-US job
ย ย ย ย }

ย ย ย ย return true; // Keep US job
ย ย ย });

ย ย ย console.log(`๐บ๏ธ Location filtering scraped jobs: ${initialScrapedCount} -> ${levelFilteredJobs.length} jobs (removed ${removedJobs.length} non-US jobs)`);
ย ย }
ย } catch (locationError) {
ย ย console.error('โ Error in location filtering scraped jobs:', locationError.message);
ย }

ย // NOW fetch jobs from companies with APIs (these typically have no descriptions)
ย const companiesWithAPIs = Object.keys(CAREER_APIS);
ย let apiJobsCount = 0;

ย // Fetch real jobs from companies with APIs
ย for (const company of companiesWithAPIs) {
ย ย try {
ย ย ย const jobs = await fetchCompanyJobs(company);
ย ย ย levelFilteredJobs.push(...jobs);
ย ย ย apiJobsCount += jobs.length;
ย ย ย console.log(`โ Added ${jobs.length} API jobs from ${company}`);
ย ย } catch (apiError) {
ย ย ย console.error(`โ Error fetching API jobs from ${company}:`, apiError.message);
ย ย }

ย ย // Be respectful with rate limiting
ย ย await delay(2000);
ย }

ย // Fetch jobs from external sources
ย 
ย
ย ย const externalJobs = await fetchSimplifyJobsData();

ย ย

ย ย levelFilteredJobs.push(...externalJobs);
ย ย
ย ย console.log(`โ Added ${externalJobs.length}  external jobs)`);
ย 

ย // Final deduplication using standardized job ID generation
ย const uniqueJobs = levelFilteredJobs.filter((job, index, self) => {
ย ย const jobId = generateJobId(job);
ย ย return index === self.findIndex((j) => generateJobId(j) === jobId);
ย });

ย console.log(`๐งน Final deduplication: ${levelFilteredJobs.length} -> ${uniqueJobs.length} jobs`);

ย // Sort by posting date (descending - latest first)
ย uniqueJobs.sort((a, b) => {
ย ย const dateA = new Date(a.job_posted_at);
ย ย const dateB = new Date(b.job_posted_at);
ย ย return dateB - dateA;
ย });

ย // Calculate scraped jobs count (total jobs minus API and external jobs)
ย const scrapedJobsCount = allJobs.length;

ย // Final summary
ย console.log(`\n๐ฏ ===== FINAL SUMMARY =====`);
ย console.log(`๐ Total unique jobs: ${uniqueJobs.length}`);
ย console.log(` ย ๐ Scraped jobs (with descriptions): ${scrapedJobsCount}`);
ย console.log(` ย ๐ API jobs (may have limited descriptions): ${apiJobsCount}`);
ย console.log(`โ REAL JOBS ONLY - No fake data!`);

ย return uniqueJobs;
}

module.exports = { fetchAllRealJobs };